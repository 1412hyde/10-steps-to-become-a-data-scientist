{
  "cells": [
    {
      "metadata": {
        "_uuid": "a8f9622945156d6337ba73c481da2de7efef7384"
      },
      "cell_type": "markdown",
      "source": "## <div style=\"text-align: center\">A Comprehensive Deep Learning Workflow with Python </div>\n\n<div style=\"text-align: center\">This <b>tutorial</b> demonstrates the basic workflow of using <b>TensorFlow</b> for <b>Deep Learning</b>. After loading the so-called <b>MNIST</b> data-set with images of hand-written digits, we define and optimize a simple mathematical model in TensorFlow. The results are then plotted and discussed.\n\nYou should be familiar with basic [linear algebra](https://www.kaggle.com/mjbahmani/linear-algebra-in-60-minutes), [Python](https://www.kaggle.com/mjbahmani/10-steps-to-become-a-data-scientist) and the Jupyter Notebook editor. It also helps if you have a basic understanding of [Machine Learning](http://https://www.kaggle.com/mjbahmani/a-comprehensive-ml-workflow-with-python) and classification.</div>\n<div style=\"text-align:center\">last update: <b>10/09/2018</b></div>\n\n\n\n>###### you may  be interested have a look at it: [**A Comprehensive ML Workflow for House Prices**](https://www.kaggle.com/mjbahmani/a-comprehensive-ml-workflow-for-house-prices)\n\n\n---------------------------------------------------------------------\nFork and run my kernels on **GiHub**  and follow me:\n> ###### [ GitHub](https://github.com/mjbahmani)\n-------------------------------------------------------------------------------------------------------------\n **I hope you find this kernel helpful and some upvotes would be very much appreciated**\n \n -----------"
    },
    {
      "metadata": {
        "_uuid": "cda11210a88d6484112cbe2c3624225328326c6a"
      },
      "cell_type": "markdown",
      "source": "## Notebook  Content\n*   1-  [Introduction](#1)\n*   2- [Machine learning workflow](#2)\n*   3- [Problem Definition](#3)\n*       3-1 [Problem feature](#4)\n*       3-2 [Aim](#5)\n*       3-3 [Variables](#6)\n*   4-[ Inputs & Outputs](#7)\n*   4-1 [Inputs ](#8)\n*   4-2 [Outputs](#9)\n*   5- [Installation](#10)\n*       5-1 [ jupyter notebook](#11)\n*       5-2[ kaggle kernel](#12)\n*       5-3 [Colab notebook](#13)\n*       5-4 [install python & packages](#14)\n*       5-5 [Loading Packages](#15)\n*   6- [Exploratory data analysis](#16)\n*       6-1 [Data Collection](#17)\n*       6-2 [Visualization](#18)\n*       6-3 [Data Preprocessing](#30)\n*       6-4 [Data Cleaning](#31)\n*   7- [Model Deployment](#32)\n*   8- [Conclusion](#53)\n*  9- [References](#54)"
    },
    {
      "metadata": {
        "_uuid": "750903cc2679d39058f56df6c6c040be02b748df"
      },
      "cell_type": "markdown",
      "source": " <a id=\"1\"></a> <br>\n## 1- Introduction\nThis is a **comprehensive DP techniques with python** data set, that I have spent for more than two months to complete it.\n\nit is clear that everyone in this community is familiar with **MNIST dataset** but if you need to review your information about the dataset please visit this [link](https://en.wikipedia.org/wiki/MNIST_database).\n\nI have tried to help  Kaggle users  how to face deep learning problems. and I think it is a great opportunity for who want to learn deep learning workflow with python completely.\n\nI am open to getting your feedback for improving this **kernel**\n"
    },
    {
      "metadata": {
        "_uuid": "e11b73b618b0f6e4335520ef80267c6d577d1ba5"
      },
      "cell_type": "markdown",
      "source": "<a id=\"2\"></a> <br>\n## 2- Deep Learning Workflow\nIf you have already read some [Deep Learning books](https://towardsdatascience.com/list-of-free-must-read-machine-learning-books-89576749d2ff). You have noticed that there are different ways to stream data into deep learning.\n\nmost of these books share the following steps:\n*   Define Problem\n*   Specify Inputs & Outputs\n*   Exploratory data analysis\n*   Data Collection\n*   Data Preprocessing\n*   Data Cleaning\n*   Visualization\n*   Model Design, Training, and Offline Evaluation\n*   Model Deployment, Online Evaluation, and Monitoring\n*   Model Maintenance, Diagnosis, and Retraining\n\n**You can see my workflow in the below image** :\n <img src=\"http://s9.picofile.com/file/8338227634/workflow.png\" />\n\n"
    },
    {
      "metadata": {
        "_uuid": "600be852c0d28e7c0c5ebb718904ab15a536342c"
      },
      "cell_type": "markdown",
      "source": "<a id=\"3\"></a> <br>\n## 3- Problem Definition\nI think one of the important things when you start a new machine learning project is Defining your problem.\n\nProblem Definition has four steps that have illustrated in the picture below:\n<img src=\"http://s8.picofile.com/file/8338227734/ProblemDefination.png\">\n<a id=\"4\"></a> <br>\n### 3-1 Problem Feature\nwe will use the classic MNIST  data set. This dataset contains information about  handwritten digits that is commonly used for training various image processing systems.\nhe MNIST database contains 60,000 training images and 10,000 testing images.\n\nHalf of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset. There have been a number of scientific papers on attempts to achieve the lowest error rate\n<a id=\"5\"></a> <br>\n### 3-2 Aim\n your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images\n<a id=\"6\"></a> <br>\n### 3-3 Variables\nEach **pixel** column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n"
    },
    {
      "metadata": {
        "_uuid": "8bb4dfebb521f83543e1d45db3559216dad8f6fb"
      },
      "cell_type": "markdown",
      "source": "<a id=\"7\"></a> <br>\n## 4- Inputs & Outputs\n<a id=\"8\"></a> <br>\n### 4-1 Inputs\nThe data files train.csv and test.csv contain gray-scale images of **hand-drawn digits**, from zero through nine.\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between **0 and 255**, inclusive.\n\nThe training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"></img>\n<a id=\"9\"></a> <br>\n### 4-2 Outputs\nyour Output is to correctly identify digits from a dataset of tens of thousands of handwritten images."
    },
    {
      "metadata": {
        "_uuid": "89ee0cda57822cd4102eadf8992c5bfe1964d557"
      },
      "cell_type": "markdown",
      "source": "<a id=\"10\"></a> <br>\n## 5-Installation\n#### Windows:\n* Anaconda (from https://www.continuum.io) is a free Python distribution for SciPy stack. It is also available for Linux and Mac.\n* Canopy (https://www.enthought.com/products/canopy/) is available as free as well as commercial distribution with full SciPy stack for Windows, Linux and Mac.\n* Python (x,y) is a free Python distribution with SciPy stack and Spyder IDE for Windows OS. (Downloadable from http://python-xy.github.io/)\n#### Linux\nPackage managers of respective Linux distributions are used to install one or more packages in SciPy stack.\n\nFor Ubuntu Users:\nsudo apt-get install python-numpy python-scipy python-matplotlibipythonipythonnotebook\npython-pandas python-sympy python-nose"
    },
    {
      "metadata": {
        "_uuid": "c1793fb141d3338bbc4300874be6ffa5cb1a9139"
      },
      "cell_type": "markdown",
      "source": "<a id=\"11\"></a> <br>\n## 5-1 Jupyter notebook\nI strongly recommend installing **Python** and **Jupyter** using the **[Anaconda Distribution](https://www.anaconda.com/download/)**, which includes Python, the Jupyter Notebook, and other commonly used packages for scientific computing and data science.\n\nFirst, download Anaconda. We recommend downloading Anaconda’s latest Python 3 version.\n\nSecond, install the version of Anaconda which you downloaded, following the instructions on the download page.\n\nCongratulations, you have installed Jupyter Notebook! To run the notebook, run the following command at the Terminal (Mac/Linux) or Command Prompt (Windows):"
    },
    {
      "metadata": {
        "_uuid": "abbd1757dde9805758a2cec47a186e31dbc29822"
      },
      "cell_type": "markdown",
      "source": "> jupyter notebook\n> "
    },
    {
      "metadata": {
        "_uuid": "8a70c253d5afa93f07a7a7e048dbb2d7812c8d10"
      },
      "cell_type": "markdown",
      "source": "<a id=\"12\"></a> <br>\n## 5-2 Kaggle Kernel\nKaggle kernel is an environment just like you use jupyter notebook, it's an **extension** of the where in you are able to carry out all the functions of jupyter notebooks plus it has some added tools like forking et al."
    },
    {
      "metadata": {
        "_uuid": "237bbe4e4509c9491ce165e3599c432b979d7b90"
      },
      "cell_type": "markdown",
      "source": "<a id=\"13\"></a> <br>\n## 5-3 Colab notebook\n**Colaboratory** is a research tool for machine learning education and research. It’s a Jupyter notebook environment that requires no setup to use.\n### 5-3-1 What browsers are supported?\nColaboratory works with most major browsers, and is most thoroughly tested with desktop versions of Chrome and Firefox.\n### 5-3-2 Is it free to use?\nYes. Colaboratory is a research project that is free to use.\n### 5-3-3 What is the difference between Jupyter and Colaboratory?\nJupyter is the open source project on which Colaboratory is based. Colaboratory allows you to use and share Jupyter notebooks with others without having to download, install, or run anything on your own computer other than a browser."
    },
    {
      "metadata": {
        "_uuid": "fbedcae8843986c2139f18dad4b5f313e6535ac5"
      },
      "cell_type": "markdown",
      "source": "<a id=\"15\"></a> <br>\n## 5-5 Loading Packages\nIn this kernel we are using the following packages:"
    },
    {
      "metadata": {
        "_uuid": "61f49281fdd8592b44c0867225f57e6fce36342c"
      },
      "cell_type": "markdown",
      "source": " <img src=\"http://s8.picofile.com/file/8338227868/packages.png\">\n Now we import all of them "
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# packages to load \n# Check the versions of libraries\n# Python version\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sys\nprint('Python: {}'.format(sys.version))\n# scipy\nimport scipy\nprint('scipy: {}'.format(scipy.__version__))\nimport numpy\n# matplotlib\nimport matplotlib\nprint('matplotlib: {}'.format(matplotlib.__version__))\n# numpy\nimport numpy as np # linear algebra\nprint('numpy: {}'.format(np.__version__))\n# pandas\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nprint('pandas: {}'.format(pd.__version__))\nimport seaborn as sns\nprint('seaborn: {}'.format(sns.__version__))\nsns.set(color_codes=True)\nimport matplotlib.pyplot as plt\nprint('matplotlib: {}'.format(matplotlib.__version__))\n%matplotlib inline\n# scikit-learn\nimport sklearn\nprint('sklearn: {}'.format(sklearn.__version__))\nimport tensorflow as tf\nprint('tensorflow: {}'.format(tf.__version__))\nnp.random.seed(2)\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nimport os\n%matplotlib inline\nsns.set(style='white', context='notebook', palette='deep')\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "04ff1a533119d589baee777c21194a951168b0c7"
      },
      "cell_type": "markdown",
      "source": "<a id=\"16\"></a> <br>\n## 6- Exploratory Data Analysis(EDA)\n In this section, you'll learn how to use graphical and numerical techniques to begin uncovering the structure of your data. \n \n* Which variables suggest interesting relationships?\n* Which observations are unusual?\n\nBy the end of the section, you'll be able to answer these questions and more, while generating graphics that are both insightful and beautiful.  then We will review analytical and statistical operations:\n\n*   5-1 Data Collection\n*   5-2 Visualization\n*   5-3 Data Preprocessing\n*   5-4 Data Cleaning\n<img src=\"http://s9.picofile.com/file/8338476134/EDA.png\">"
    },
    {
      "metadata": {
        "_uuid": "cedecea930b278f86292367cc28d2996a235a169"
      },
      "cell_type": "markdown",
      "source": "<a id=\"17\"></a> <br>\n## 6-1 Data Collection\n**Data collection** is the process of gathering and measuring data, information or any variables of interest in a standardized and established manner that enables the collector to answer or test hypothesis and evaluate outcomes of the particular collection.[techopedia]\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9269ae851b744856bce56840637030a16a5877e1"
      },
      "cell_type": "code",
      "source": "# import Dataset to play with it\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "58ed9c838069f54de5cf90b20a774c3e236149b3"
      },
      "cell_type": "markdown",
      "source": "**<< Note 1 >>**\n\n* Each row is an observation (also known as : sample, example, instance, record)\n* Each column is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate)"
    },
    {
      "metadata": {
        "_uuid": "7b5fd1034cd591ebd29fba1c77d342ec2b408d13"
      },
      "cell_type": "markdown",
      "source": "After loading the data via **pandas**, we should checkout what the content is, description and via the following:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "edd043f8feb76cfe51b79785302ca4936ceb7b51"
      },
      "cell_type": "code",
      "source": "type(train)\ntype(test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "581b90e6a869c3793472c7edd59091d6d6342fb2"
      },
      "cell_type": "markdown",
      "source": "## 6-1-1 Statistical Summary\n1- Dimensions of the dataset.\n\n2- Peek at the data itself.\n\n3- Statistical summary of all attributes.\n\n4- Breakdown of the data by the class variable.[7]\n\nDon’t worry, each look at the data is **one command**. These are useful commands that you can use again and again on future projects."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4b45251be7be77333051fe738639104ae1005fa5"
      },
      "cell_type": "code",
      "source": "# shape\nprint(train.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4b45251be7be77333051fe738639104ae1005fa5"
      },
      "cell_type": "code",
      "source": "# shape\nprint(test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c64e9d3e0bf394fb833de94a0fc5c34f69fce24c"
      },
      "cell_type": "code",
      "source": "#columns*rows\ntrain.size",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c64e9d3e0bf394fb833de94a0fc5c34f69fce24c"
      },
      "cell_type": "code",
      "source": "#columns*rows\ntest.size",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "277e1998627d6a3ddeff4e913a6b8c3dc81dec96"
      },
      "cell_type": "markdown",
      "source": "\nWe can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property.\n\nYou should see 42000 instances and 785 attributes for train.csv"
    },
    {
      "metadata": {
        "_uuid": "95ee5e18f97bc410df1e54ac74e32cdff2b30755"
      },
      "cell_type": "markdown",
      "source": "for getting some information about the dataset you can use **info()** command"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca840f02925751186f87e402fcb5f637ab1ab8a0"
      },
      "cell_type": "code",
      "source": "print(train.info())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ae08b544a8d4202c7d0a47ec83d685e81c91a66d"
      },
      "cell_type": "markdown",
      "source": "to check the first 5 rows of the data set, we can use head(5)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5899889553c3416b27e93efceddb106eb71f5156"
      },
      "cell_type": "code",
      "source": "train.head(5) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1150b6ac3d82562aefd5c64f9f01accee5eace4d"
      },
      "cell_type": "markdown",
      "source": "to check out last 5 row of the data set, we use tail() function"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "79339442ff1f53ae1054d794337b9541295d3305"
      },
      "cell_type": "code",
      "source": "train.tail() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2c288c3dc8656a872a8529368812546e434d3a22"
      },
      "cell_type": "markdown",
      "source": "to pop up 5 random rows from the data set, we can use **sample(5)**  function"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "09eb18d1fcf4a2b73ba2f5ddce99dfa521681140"
      },
      "cell_type": "code",
      "source": "train.sample(5) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c8a1cc36348c68fb98d6cb28aa9919fc5f2892f3"
      },
      "cell_type": "markdown",
      "source": "to give a statistical summary about the dataset, we can use **describe()"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f7211e96627b9a81c5b620a9ba61446f7719ea3"
      },
      "cell_type": "code",
      "source": "train.describe() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6af5638e71e4f6d0bee777523245237744a48294"
      },
      "cell_type": "markdown",
      "source": "##  Data preparation"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c582e1c7e3ef4f5cf8557af9db786c4a51df1a50"
      },
      "cell_type": "code",
      "source": "Y_train = train[\"label\"]\n\n# Drop 'label' column\nX_train = train.drop(labels = [\"label\"],axis = 1) \n\n# free some space\ndel train \n\ng = sns.countplot(Y_train)\n\nY_train.value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "91dda1f631cf4ed362162501aaaac6d19cfd6cc7"
      },
      "cell_type": "markdown",
      "source": "<a id=\"30\"></a> <br>\n## 6-3 Data Preprocessing\n**Data preprocessing** refers to the transformations applied to our data before feeding it to the algorithm.\n \nData Preprocessing is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.\nthere are plenty of steps for data preprocessing and we just listed some of them :\n* removing Target column (id)\n* Sampling (without replacement)\n* Making part of iris unbalanced and balancing (with undersampling and SMOTE)\n* Introducing missing values and treating them (replacing by average values)\n* Noise filtering\n* Data discretization\n* Normalization and standardization\n* PCA analysis\n* Feature selection (filter, embedded, wrapper)"
    },
    {
      "metadata": {
        "_uuid": "788b89478726250719c5482bab76e665438f629b"
      },
      "cell_type": "markdown",
      "source": "# New Chapter Coming Soon"
    },
    {
      "metadata": {
        "_uuid": "cf3679a51c72dbe2d2549b5fe97e4ac5f1fa0fa0"
      },
      "cell_type": "markdown",
      "source": "you can follow and fork my work  in **GitHub**:\n> ###### [ GitHub](https://github.com/mjbahmani)\n\n\n--------------------------------------\n\n **I hope you find this kernel helpful and some upvotes would be very much appreciated**\n "
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "<a id=\"54\"></a> <br>\n\n-----------\n\n# 9- References\n* [3] [https://skymind.ai/wiki/machine-learning-workflow](https://skymind.ai/wiki/machine-learning-workflow)\n* [4] [keras](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6)\n* [5] [Problem-define](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)\n* [6] [Sklearn](http://scikit-learn.org/)\n* [7] [machine-learning-in-python-step-by-step](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)\n* [8] [Data Cleaning](http://wp.sigmod.org/?p=2288)\n* [9] [Kaggle kernel that I use it](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6)\n\n\n\n-------------\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}